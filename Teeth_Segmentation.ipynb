{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH6Z4tJJYi33"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVl_uwBmSNVM"
      },
      "source": [
        "### Downloading the Teeth Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSh0F6mKYi33"
      },
      "source": [
        "First the dataset need to be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-03T12:54:42.172022Z",
          "iopub.status.busy": "2024-10-03T12:54:42.171594Z",
          "iopub.status.idle": "2024-10-03T12:54:42.176491Z",
          "shell.execute_reply": "2024-10-03T12:54:42.175412Z",
          "shell.execute_reply.started": "2024-10-03T12:54:42.171973Z"
        },
        "id": "gCW90FvnYi34",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets tqdm -q\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fXkqU5wG6iaw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 2 files: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded completely.\n",
            "Total size of downloaded files: 86.07 MB\n",
            "Dataset has been saved at: [c:\\Users\\raksh\\Teeth-Segmentation-using-Unet\\Main_teeth_dataset]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_id=\"RayanAi/Main_teeth_dataset\"\n",
        "# Set the local directory where you want to store the dataset\n",
        "local_dataset_dir = \"./Main_teeth_dataset\"  # You can change this path to your desired location\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "# Suppress the output by redirecting it to os.devnull\n",
        "with open(os.devnull, 'w') as fnull:\n",
        "    # Save the original stdout\n",
        "    original_stdout = sys.stdout\n",
        "    try:\n",
        "        # Redirect stdout to devnull to suppress output\n",
        "        sys.stdout = fnull\n",
        "        # Download the dataset and store it locally\n",
        "        snapshot_download(repo_id=dataset_id, local_dir=local_dataset_dir, repo_type=\"dataset\")\n",
        "    finally:\n",
        "        # Restore the original stdout\n",
        "        sys.stdout = original_stdout\n",
        "\n",
        "# Print message when download is complete\n",
        "print(\"Dataset downloaded completely.\")\n",
        "\n",
        "# Calculate and print the total size of the downloaded files\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(local_dataset_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "\n",
        "# Convert size to MB and print\n",
        "print(f\"Total size of downloaded files: {total_size / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "# Get the absolute path of the dataset directory and print it\n",
        "dataset_abs_path = os.path.abspath(local_dataset_dir)\n",
        "print(f\"Dataset has been saved at: [{dataset_abs_path}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SFB6bWJt8pfJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q ./Main_teeth_dataset/Main_teeth_dataset.zip -d ./Main_teeth_dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Hse5G8hOQ9"
      },
      "source": [
        "### Handling Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x7BAty5ZLdE3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "#Define the split ratio\n",
        "split_ratio = 0.8\n",
        "\n",
        "#Write a Dataset function called TeethSegmentationDataset\n",
        "\n",
        "\n",
        "#Write a Dataset function called TeethSegmentationDataset\n",
        "class TeethSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir: str, mask_dir: str, transform: A.Compose, dataset_type: str = 'Train', noisy_masks: list = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory path containing input images.\n",
        "            mask_dir (str): Directory path containing corresponding segmentation masks.\n",
        "            transform (callable): Optional transformation to be applied to both the image and the mask. Use A.Compose. Use ToTensorV2()\n",
        "            dataset_type (str, optional): Type of dataset, e.g., 'Train' or 'Test'. Defaults to 'Train'.\n",
        "            noisy_masks (list, optional): Provide a list of names for images you want to be excluded from dataset\n",
        "        \"\"\"\n",
        "        # Initialize paths and transformation\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.dataset_type = dataset_type\n",
        "\n",
        "        # List of all images and masks\n",
        "        self.images = os.listdir(image_dir)\n",
        "        self.masks = os.listdir(mask_dir)\n",
        "        # Filter out noisy masks if provided\n",
        "        if noisy_masks:\n",
        "            self.images = [img for img in self.images if img not in noisy_masks]\n",
        "            self.masks = [mask for mask in self.masks if mask not in noisy_masks]\n",
        "        number_of_samples = len(self.images)\n",
        "\n",
        "        if dataset_type == 'Train':\n",
        "            self.images = self.images[:int(number_of_samples*split_ratio)]\n",
        "            self.masks = self.masks[:int(number_of_samples*split_ratio)]\n",
        "        elif dataset_type == 'Test':\n",
        "            self.images = os.listdir(image_dir)[int(number_of_samples*split_ratio):]\n",
        "            self.masks = os.listdir(mask_dir)[int(number_of_samples*split_ratio):]\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: The total number of image-mask pairs in the designated dataset split.\n",
        "        \"\"\"\n",
        "        # Return the length of the dataset (number of images)\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index of the image-mask pair to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the image and its corresponding one-hot encoded mask.\n",
        "                - image (torch.Tensor): Transformed image tensor.\n",
        "                - onehot_mask (torch.Tensor): One-hot encoded mask tensor for segmentation.\n",
        "        \"\"\"\n",
        "        # Load the image and mask\n",
        "        image_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index])\n",
        "\n",
        "        # Load image and mask as grayscale\n",
        "        image = np.array(Image.open(image_path).convert(\"L\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
        "        transformed = self.transform(image=image, mask=mask)\n",
        "        image = transformed['image']\n",
        "        mask = transformed['mask']\n",
        "        binary_mask = mask.unsqueeze(2)>0\n",
        "        binary_mask = binary_mask.permute(2, 0, 1).float()\n",
        "\n",
        "        return image, binary_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-10-03T12:55:46.601863Z",
          "iopub.status.busy": "2024-10-03T12:55:46.601464Z",
          "iopub.status.idle": "2024-10-03T12:55:50.907443Z",
          "shell.execute_reply": "2024-10-03T12:55:50.906555Z",
          "shell.execute_reply.started": "2024-10-03T12:55:46.601819Z"
        },
        "id": "YjQTJPp_Yi35",
        "outputId": "e7672494-cbd0-447b-89e8-97689e45d4b8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "######################augmenters######################\n",
        "#You may want to alter this part\n",
        "augmenter = A.Compose([\n",
        "    A.Normalize(mean=(0.485,), std=(0.229,), max_pixel_value=255.0),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "######################################################\n",
        "\n",
        "######################################################\n",
        "#Don't touch this part\n",
        "test_augmenter = A.Compose([\n",
        "    A.Normalize(mean=(0.485,), std=(0.229,), max_pixel_value=255.0),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "######################################################\n",
        "\n",
        "train_dataset = TeethSegmentationDataset(\n",
        "    image_dir=\"./Main_teeth_dataset/Main_teeth_dataset/images\",\n",
        "    mask_dir=\"./Main_teeth_dataset/Main_teeth_dataset/labels\",\n",
        "    transform=augmenter,\n",
        "    dataset_type='Train',\n",
        "\n",
        ")\n",
        "\n",
        "test_dataset = TeethSegmentationDataset(\n",
        "    image_dir= \"./Main_teeth_dataset/Main_teeth_dataset/images\",\n",
        "    mask_dir=\"./Main_teeth_dataset/Main_teeth_dataset/labels\",\n",
        "    transform=test_augmenter,\n",
        "    dataset_type='Test',\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-03T12:55:50.909181Z",
          "iopub.status.busy": "2024-10-03T12:55:50.908756Z",
          "iopub.status.idle": "2024-10-03T12:55:50.914666Z",
          "shell.execute_reply": "2024-10-03T12:55:50.913661Z",
          "shell.execute_reply.started": "2024-10-03T12:55:50.909147Z"
        },
        "id": "BIDBT-kWYi35",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 482\n",
            "Test dataset size: 121\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size=8\n",
        "num_workers=0 # Increase this if you have a powerfull cpu\n",
        "dataloaders = {\n",
        "  'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
        "  'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "}\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UssW6iJ7Yi36"
      },
      "source": [
        "# Segmentation model\n",
        "In this part you should design a segmentation model. If you have defined any functions used to define your model, you should upload it along the model code.\n",
        "\n",
        "Your model shouldn't take any inputs or produce outputs when instantiating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-uA9CsrYi36"
      },
      "source": [
        "<font color='red'>Important: You can only use functions availble in `torch` and `torchvision`.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-03T12:55:52.845729Z",
          "iopub.status.busy": "2024-10-03T12:55:52.845422Z",
          "iopub.status.idle": "2024-10-03T12:55:52.866619Z",
          "shell.execute_reply": "2024-10-03T12:55:52.865746Z",
          "shell.execute_reply.started": "2024-10-03T12:55:52.845670Z"
        },
        "id": "kaXDqMPr8b1b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        in_channels = 1\n",
        "        out_channels = 1\n",
        "        # Reduce number of filters to keep model size under control\n",
        "        self.encoder1 = self.conv_block(in_channels, 32)\n",
        "        self.encoder2 = self.conv_block(32, 64)\n",
        "        self.encoder3 = self.conv_block(64, 128)\n",
        "        self.encoder4 = self.conv_block(128, 256)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(256, 512)\n",
        "\n",
        "        # Expanding path with reduced number of filters\n",
        "        self.decoder4 = self.upconv_block(512, 256)\n",
        "        self.decoder3 = self.upconv_block(256, 128)\n",
        "        self.decoder2 = self.upconv_block(128, 64)\n",
        "        self.decoder1 = self.upconv_block(64, 32)\n",
        "\n",
        "        # Output layer\n",
        "        self.final_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    #######DO NOT CHANGE THIS PART########\n",
        "    def init(self):\n",
        "        self.load_state_dict(torch.load('model.pth', weights_only=True))\n",
        "    ######################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        This method defines the forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (tensor): The input tensor, in the shape of (batch_size,1,512,512).\n",
        "\n",
        "        Returns:\n",
        "            mask (tensor): The output tensor logits, in the shape of (batch_size,1,512,512).\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2))\n",
        "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2))\n",
        "        enc4 = self.encoder4(F.max_pool2d(enc3, kernel_size=2))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(F.max_pool2d(enc4, kernel_size=2))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.decoder4(bottleneck)\n",
        "        dec4 = self.center_crop_and_concat(enc4, dec4)  # Skip connection\n",
        "        dec3 = self.decoder3(dec4)\n",
        "        dec3 = self.center_crop_and_concat(enc3, dec3)  # Skip connection\n",
        "        dec2 = self.decoder2(dec3)\n",
        "        dec2 = self.center_crop_and_concat(enc2, dec2)  # Skip connection\n",
        "        dec1 = self.decoder1(dec2)\n",
        "        dec1 = self.center_crop_and_concat(enc1, dec1)  # Skip connection\n",
        "\n",
        "        # Output\n",
        "        mask = self.final_conv(dec1)\n",
        "        return mask\n",
        "\n",
        "    def center_crop_and_concat(self, enc, dec):\n",
        "        # Get dimensions of the encoder and decoder feature maps\n",
        "        enc_size = enc.size()[2:]  # (H, W) of the encoder feature map\n",
        "        dec_size = dec.size()[2:]  # (H, W) of the decoder feature map\n",
        "\n",
        "        # Calculate the required padding for each dimension\n",
        "        diff_y = enc_size[0] - dec_size[0]\n",
        "        diff_x = enc_size[1] - dec_size[1]\n",
        "\n",
        "        # Pad the decoder tensor to match the encoder size if needed\n",
        "        dec = F.pad(dec, [diff_x // 2, diff_x - diff_x // 2,\n",
        "                      diff_y // 2, diff_y - diff_y // 2])\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        return torch.cat((enc, dec), dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1-MwrpM_DlqD"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Given transposed=1, weight of size [256, 128, 2, 2], expected input[8, 512, 64, 64] to have 256 channels, but got 512 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[11], line 72\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(bottleneck)\n\u001b[0;32m     71\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop_and_concat(enc4, dec4)  \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop_and_concat(enc3, dec3)  \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n\u001b[0;32m     74\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2(dec3)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:1162\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m   1151\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1152\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1154\u001b[0m     output_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 128, 2, 2], expected input[8, 512, 64, 64] to have 256 channels, but got 512 channels instead"
          ]
        }
      ],
      "source": [
        "model = Model().to(device)\n",
        "\n",
        "image = next(iter(dataloaders['train']))[0].to(device)\n",
        "out = model(image)\n",
        "print(image.shape)\n",
        "print(out.shape)\n",
        "assert image.shape == (batch_size, 1, 512, 512), \"You shouldn't change the size of the image\"\n",
        "assert out.shape == (batch_size, 1, 512, 512), \"The output of your model do not have correct dimensions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Hn1LRJYi38"
      },
      "source": [
        "## Dice Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmjpzwarYi38"
      },
      "source": [
        "Here is the dice score function. You model is evaluated based on the score from this function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-03T12:55:58.421930Z",
          "iopub.status.busy": "2024-10-03T12:55:58.421325Z",
          "iopub.status.idle": "2024-10-03T12:55:58.428953Z",
          "shell.execute_reply": "2024-10-03T12:55:58.427975Z",
          "shell.execute_reply.started": "2024-10-03T12:55:58.421887Z"
        },
        "id": "vG_qOEmpYi38",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def dice_score(pred: torch.Tensor, target_mask: torch.Tensor, epsilon: float = 1e-6) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Dice score between the predicted and target segmentation masks.\n",
        "\n",
        "    Args:\n",
        "        pred (torch.Tensor): The predicted mask tensor, with values in range [0, 1].\n",
        "        target_one_target_maskhot (torch.Tensor): The ground truth mask.\n",
        "        epsilon (float, optional): A small value to avoid division by zero. Defaults to 1e-6.\n",
        "\n",
        "    Returns:\n",
        "        float: The Dice score, a similarity metric between 0 and 1.\n",
        "    \"\"\"\n",
        "    pred = pred>0\n",
        "    pred_flat = pred.contiguous().view(pred.shape[0], pred.shape[1], -1)\n",
        "    target_flat = target_mask.contiguous().view(target_mask.shape[0], target_mask.shape[1], -1)\n",
        "\n",
        "    intersection = (pred_flat * target_flat).sum(dim=-1)\n",
        "    union = pred_flat.sum(dim=-1) + target_flat.sum(dim=-1)\n",
        "\n",
        "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
        "\n",
        "    dice_mean = dice.mean(dim=1)\n",
        "\n",
        "    return dice_mean.mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnvf2rZZYi39"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the Dice Loss function\n",
        "def dice_loss(pred, target, smooth=1.):\n",
        "    intersection = (pred * target).sum()\n",
        "    return 1 - (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "\n",
        "# Combined Loss: BCE + Dice\n",
        "criterion = lambda pred, target: nn.BCEWithLogitsLoss()(pred, target) + dice_loss(torch.sigmoid(pred), target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-03T12:55:52.867995Z",
          "iopub.status.busy": "2024-10-03T12:55:52.867740Z",
          "iopub.status.idle": "2024-10-03T12:55:58.419880Z",
          "shell.execute_reply": "2024-10-03T12:55:58.419005Z",
          "shell.execute_reply.started": "2024-10-03T12:55:52.867967Z"
        },
        "id": "nVGqqpruBtJE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Define your optimizer and loss function\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lSujS8GU5O8j"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize_mask(inputs: torch.Tensor, masks: torch.Tensor, outputs: torch.Tensor):\n",
        "    # Convert tensors to numpy for visualization\n",
        "    sample_index = 0  # Index of the sample to visualize\n",
        "    channel = 0\n",
        "    print(f'Dice score is {dice_score(outputs[sample_index:sample_index+1,channel:channel+1],masks[sample_index:sample_index+1,channel:channel+1])}')\n",
        "\n",
        "    inputs_np = inputs.cpu().numpy()\n",
        "    masks_np = masks.cpu().numpy()\n",
        "    outputs_np = outputs.detach().cpu().numpy()\n",
        "\n",
        "    # Choose a sample to visualize\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(inputs_np[sample_index].transpose(1, 2, 0), cmap='gray')  # Assuming inputs are in CxHxW format\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(masks_np[sample_index, channel], cmap='gray')  # Display the first channel of the mask\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(outputs_np[sample_index, channel]>0, cmap='gray')  # Display the first channel of the output\n",
        "    plt.title(\"Model Output Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkTdAZTZYi39"
      },
      "source": [
        "The `train_model` function implements a simple training loop that iterates over a specified number of epochs. In each iteration, the model is trained on the training set and then evaluated on the validation set using the Dice score as the performance metric. The function returns the model as it is at the final epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-03T12:55:58.455847Z",
          "iopub.status.busy": "2024-10-03T12:55:58.455167Z"
        },
        "id": "TlOg4FmFPI4T",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given transposed=1, weight of size [256, 128, 2, 2], expected input[8, 512, 64, 64] to have 256 channels, but got 512 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[17], line 62\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs,masks)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Backward + optimize only if in training phase\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[11], line 72\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(bottleneck)\n\u001b[0;32m     71\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop_and_concat(enc4, dec4)  \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop_and_concat(enc3, dec3)  \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n\u001b[0;32m     74\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2(dec3)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\raksh\\anaconda3\\envs\\unet_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:1162\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m   1151\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1152\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1154\u001b[0m     output_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [256, 128, 2, 2], expected input[8, 512, 64, 64] to have 256 channels, but got 512 channels instead"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "model = model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Training function with visualization support\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    dataloaders: dict[str, DataLoader],\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    num_epochs: int = 25\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Trains the model over a specified number of epochs using the given data loaders,\n",
        "    criterion (loss function), and optimizer.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to be trained.\n",
        "        dataloaders (dict[str, DataLoader]): A dictionary containing 'train' and 'test' data loaders.\n",
        "        criterion (nn.Module): The loss function to be used for training.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer used to adjust model parameters.\n",
        "        num_epochs (int, optional): Number of epochs for training. Defaults to 25.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: The trained model.\n",
        "    \"\"\"\n",
        "    since = time.time()\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    dice_scores_epoch = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        dice_scores = []\n",
        "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, masks in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                    loss = criterion(outputs,masks)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    else:\n",
        "                        dice_scores.append(dice_score(outputs, masks))\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            if phase == 'train':\n",
        "                train_losses.append(epoch_loss)\n",
        "            else:\n",
        "                test_losses.append(epoch_loss)\n",
        "                dice_scores_epoch.append(torch.tensor(dice_scores).mean().item())\n",
        "\n",
        "                visualize_mask(inputs, masks, outputs)\n",
        "\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "            if phase == 'test':\n",
        "                print(f'Dice score: {torch.tensor(dice_scores).mean()}')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "    # Plot the results\n",
        "    epochs_range = range(num_epochs)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(epochs_range, test_losses, label=\"Test Loss\")\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Training and Test Loss\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, dice_scores_epoch, label=\"Dice Score\", color=\"green\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.title(\"Dice Score\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model, dataloaders, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DOgSabiqrxe4"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30775,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "unet_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
